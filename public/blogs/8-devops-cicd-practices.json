{
  "id": "8",
  "slug": "devops-cicd-practices",
  "title": {
    "tr": "DevOps Kültürü ve CI/CD Pratikleri",
    "en": "DevOps Culture and CI/CD Practices"
  },
  "excerpt": {
    "tr": "DevOps kültürünün önemi ve modern CI/CD deployment pratikleri.",
    "en": "The importance of DevOps culture and modern CI/CD deployment practices."
  },
  "content": {
    "tr": "# DevOps Kültürü ve CI/CD Pratikleri\n\nDevOps, yazılım geliştirme ve IT operasyonları arasındaki duvarları yıkan bir kültürel değişimi temsil ediyor. 2024 yılında, GitOps, Infrastructure as Code (IaC) ve cloud-native teknolojilerin yaygınlaşmasıyla DevOps pratikleri yeni bir olgunluk seviyesine ulaştı. Bu kapsamlı yazıda, modern DevOps kültürünü, CI/CD pipeline tasarımını, container orchestration'ı, monitoring stratejilerini ve DevSecOps yaklaşımlarını derinlemesine inceleyeceğiz.\n\n## DevOps: Kültürel Dönüşümün Temelleri\n\nDevOps, sadece araçlar ve süreçler değil, öncelikle bir mindset değişimi. Google'ın State of DevOps Report 2024'e göre, elite performer organizasyonlar:\n- Kod deployment'ı 973x daha sık yapıyor\n- Lead time'ı 6570x daha kısa\n- Failure rate'i 3x daha düşük\n- Recovery time'ı 6570x daha hızlı\n\n**DevOps'un Üç Yolu (The Three Ways):**\n1. **Flow:** Development'tan Operations'a değer akışının optimizasyonu\n2. **Feedback:** Hızlı ve sürekli geri bildirim döngüleri\n3. **Continuous Learning:** Deneme, öğrenme ve iyileştirme kültürü\n\n![DevOps Culture](https://images.unsplash.com/photo-1519389950473-47ba0277781c?w=800&q=80)\n\n## Modern CI/CD Pipeline Tasarımı\n\nContinuous Integration ve Continuous Delivery, modern yazılım geliştirmenin bel kemiği. Spotify, günde 1000'den fazla production deployment yaparak, CI/CD mükemmelliğinin örneğini sergiliyor.\n\n### 1. Source Code Management\n\n```yaml\n# .gitlab-ci.yml - Modern GitLab CI/CD Pipeline\nstages:\n  - validate\n  - build\n  - test\n  - security\n  - deploy\n  - monitor\n\nvariables:\n  DOCKER_DRIVER: overlay2\n  DOCKER_TLS_CERTDIR: \"/certs\"\n  KUBERNETES_NAMESPACE: production\n\n# Code quality and validation\ncode-quality:\n  stage: validate\n  image: node:16-alpine\n  script:\n    - npm ci --audit-level=high\n    - npm run lint\n    - npm run prettier:check\n    - npm run type-check\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n\n# Multi-arch Docker build\nbuild-image:\n  stage: build\n  image: docker:latest\n  services:\n    - docker:dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n    - docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n    - docker buildx create --name multi-arch --driver docker-container --use\n  script:\n    - |\n      docker buildx build \\\n        --platform linux/amd64,linux/arm64 \\\n        --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA \\\n        --tag $CI_REGISTRY_IMAGE:latest \\\n        --cache-from type=registry,ref=$CI_REGISTRY_IMAGE:buildcache \\\n        --cache-to type=registry,ref=$CI_REGISTRY_IMAGE:buildcache,mode=max \\\n        --push .\n  only:\n    - main\n    - develop\n```\n\n### 2. Automated Testing Strategy\n\nModern test piramidi, geleneksel yaklaşımdan farklı:\n\n```javascript\n// Jest test configuration with coverage thresholds\nmodule.exports = {\n  preset: 'ts-jest',\n  testEnvironment: 'node',\n  coverageThreshold: {\n    global: {\n      branches: 80,\n      functions: 80,\n      lines: 80,\n      statements: 80\n    }\n  },\n  testMatch: [\n    '**/__tests__/**/*.test.ts',\n    '**/*.spec.ts'\n  ],\n  setupFilesAfterEnv: ['<rootDir>/test/setup.ts'],\n  globalSetup: '<rootDir>/test/globalSetup.ts',\n  globalTeardown: '<rootDir>/test/globalTeardown.ts'\n};\n\n// Parallel E2E testing with Playwright\n// playwright.config.ts\nimport { defineConfig, devices } from '@playwright/test';\n\nexport default defineConfig({\n  testDir: './e2e',\n  fullyParallel: true,\n  forbidOnly: !!process.env.CI,\n  retries: process.env.CI ? 2 : 0,\n  workers: process.env.CI ? 4 : undefined,\n  reporter: [\n    ['html'],\n    ['junit', { outputFile: 'test-results/junit.xml' }],\n    ['json', { outputFile: 'test-results/results.json' }]\n  ],\n  use: {\n    baseURL: process.env.BASE_URL || 'http://localhost:3000',\n    trace: 'on-first-retry',\n    screenshot: 'only-on-failure',\n    video: 'retain-on-failure'\n  },\n  projects: [\n    {\n      name: 'chromium',\n      use: { ...devices['Desktop Chrome'] },\n    },\n    {\n      name: 'firefox',\n      use: { ...devices['Desktop Firefox'] },\n    },\n    {\n      name: 'webkit',\n      use: { ...devices['Desktop Safari'] },\n    },\n    {\n      name: 'Mobile Chrome',\n      use: { ...devices['Pixel 5'] },\n    },\n  ],\n});\n```\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0b6lwYQiFuE\" frameborder=\"0\" allowfullscreen></iframe>\n\n## Infrastructure as Code (IaC)\n\nIaC, infrastructure management'ı kod haline getirerek version control, peer review ve automated testing avantajlarını infrastructure'a taşıyor.\n\n### 1. Terraform ile Multi-Cloud Infrastructure\n\n```hcl\n# main.tf - Multi-cloud Kubernetes deployment\nterraform {\n  required_version = \">= 1.5.0\"\n  \n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~> 3.0\"\n    }\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~> 5.0\"\n    }\n  }\n  \n  backend \"s3\" {\n    bucket         = \"terraform-state-prod\"\n    key            = \"infrastructure/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n\n# EKS Cluster Module\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~> 19.0\"\n\n  cluster_name    = var.cluster_name\n  cluster_version = \"1.28\"\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  # Enable IRSA\n  enable_irsa = true\n\n  # Addons\n  eks_addons = {\n    coredns = {\n      most_recent = true\n    }\n    kube-proxy = {\n      most_recent = true\n    }\n    vpc-cni = {\n      most_recent = true\n      configuration_values = jsonencode({\n        env = {\n          ENABLE_PREFIX_DELEGATION = \"true\"\n          WARM_PREFIX_TARGET       = \"1\"\n        }\n      })\n    }\n    aws-ebs-csi-driver = {\n      most_recent = true\n    }\n  }\n\n  # Node groups\n  eks_managed_node_groups = {\n    general = {\n      desired_size = 3\n      min_size     = 1\n      max_size     = 10\n\n      instance_types = [\"t3.medium\"]\n      \n      k8s_labels = {\n        Environment = \"production\"\n        GithubRepo  = \"terraform-aws-eks\"\n      }\n    }\n    \n    spot = {\n      desired_size = 2\n      min_size     = 0\n      max_size     = 5\n      \n      capacity_type  = \"SPOT\"\n      instance_types = [\"t3.medium\", \"t3a.medium\"]\n      \n      taints = [{\n        key    = \"workload\"\n        value  = \"spot\"\n        effect = \"NO_SCHEDULE\"\n      }]\n    }\n  }\n\n  tags = var.tags\n}\n```\n\n### 2. GitOps ile Declarative Deployments\n\n```yaml\n# argocd-application.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: production-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/company/k8s-configs\n    targetRevision: HEAD\n    path: overlays/production\n    kustomize:\n      images:\n        - myapp:v2.1.0\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n      allowEmpty: false\n    syncOptions:\n      - CreateNamespace=true\n      - PrunePropagationPolicy=foreground\n      - PruneLast=true\n    retry:\n      limit: 5\n      backoff:\n        duration: 5s\n        factor: 2\n        maxDuration: 3m\n  revisionHistoryLimit: 10\n```\n\n## Container Orchestration ve Service Mesh\n\n### 1. Kubernetes Advanced Patterns\n\n```yaml\n# Horizontal Pod Autoscaler with custom metrics\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: app\n  minReplicas: 3\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  - type: Pods\n    pods:\n      metric:\n        name: http_requests_per_second\n      target:\n        type: AverageValue\n        averageValue: \"1000\"\n  - type: External\n    external:\n      metric:\n        name: queue_depth\n        selector:\n          matchLabels:\n            queue: worker-jobs\n      target:\n        type: Value\n        value: \"30\"\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 15\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 15\n      - type: Pods\n        value: 4\n        periodSeconds: 15\n      selectPolicy: Max\n```\n\n### 2. Istio Service Mesh\n\n```yaml\n# Virtual Service for canary deployment\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: app-vs\nspec:\n  hosts:\n  - app.example.com\n  gateways:\n  - app-gateway\n  http:\n  - match:\n    - headers:\n        x-version:\n          exact: canary\n    route:\n    - destination:\n        host: app\n        subset: v2\n  - route:\n    - destination:\n        host: app\n        subset: v1\n      weight: 90\n    - destination:\n        host: app\n        subset: v2\n      weight: 10\n    fault:\n      delay:\n        percentage:\n          value: 0.1\n        fixedDelay: 5s\n---\n# Circuit Breaker configuration\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: app-dr\nspec:\n  host: app\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 100\n      http:\n        http1MaxPendingRequests: 100\n        http2MaxRequests: 100\n        maxRequestsPerConnection: 2\n    outlierDetection:\n      consecutiveErrors: 5\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n      minHealthPercent: 50\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n```\n\n![Kubernetes Architecture](https://images.unsplash.com/photo-1667372393119-3d4c48d07fc9?w=800&q=80)\n\n## Observability ve Monitoring\n\nModern sistemlerde monitoring yetersiz, observability gerekli. \"Bilinmeyen bilinmeyenleri\" keşfedebilmek için sistemin internal state'ini anlamak kritik.\n\n### 1. Prometheus ve Grafana Stack\n\n```yaml\n# PrometheusRule for SLO monitoring\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: slo-rules\n  labels:\n    prometheus: kube-prometheus\nspec:\n  groups:\n  - name: slo.rules\n    interval: 30s\n    rules:\n    # Error rate SLO\n    - record: slo:error_rate\n      expr: |\n        sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service)\n        /\n        sum(rate(http_requests_total[5m])) by (service)\n    \n    # Latency SLO (95th percentile)\n    - record: slo:latency_p95\n      expr: |\n        histogram_quantile(0.95,\n          sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)\n        )\n    \n    # Availability SLO\n    - record: slo:availability\n      expr: |\n        1 - (\n          sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service)\n          /\n          sum(rate(http_requests_total[5m])) by (service)\n        )\n    \n    # SLO burn rate alerts\n    - alert: SLOErrorRateTooHigh\n      expr: |\n        (\n          slo:error_rate > 0.01\n          and\n          sum(rate(http_requests_total[5m])) by (service) > 10\n        )\n      for: 5m\n      labels:\n        severity: critical\n        team: platform\n      annotations:\n        summary: \"Error rate exceeds SLO for {{ $labels.service }}\"\n        description: \"Error rate is {{ $value | humanizePercentage }} (SLO: 1%)\"\n```\n\n### 2. Distributed Tracing\n\n```go\n// OpenTelemetry instrumentation in Go\npackage main\n\nimport (\n    \"context\"\n    \"go.opentelemetry.io/otel\"\n    \"go.opentelemetry.io/otel/attribute\"\n    \"go.opentelemetry.io/otel/exporters/jaeger\"\n    \"go.opentelemetry.io/otel/sdk/resource\"\n    sdktrace \"go.opentelemetry.io/otel/sdk/trace\"\n    semconv \"go.opentelemetry.io/otel/semconv/v1.17.0\"\n    \"go.opentelemetry.io/otel/trace\"\n)\n\nfunc initTracer() (*sdktrace.TracerProvider, error) {\n    exporter, err := jaeger.New(jaeger.WithCollectorEndpoint(\n        jaeger.WithEndpoint(\"http://jaeger:14268/api/traces\"),\n    ))\n    if err != nil {\n        return nil, err\n    }\n\n    tp := sdktrace.NewTracerProvider(\n        sdktrace.WithBatcher(exporter),\n        sdktrace.WithResource(resource.NewWithAttributes(\n            semconv.SchemaURL,\n            semconv.ServiceName(\"api-service\"),\n            semconv.ServiceVersion(\"v1.0.0\"),\n            attribute.String(\"environment\", \"production\"),\n        )),\n    )\n\n    otel.SetTracerProvider(tp)\n    return tp, nil\n}\n\n// Instrumented HTTP handler\nfunc handleRequest(w http.ResponseWriter, r *http.Request) {\n    ctx := r.Context()\n    tracer := otel.Tracer(\"api-service\")\n    \n    ctx, span := tracer.Start(ctx, \"handleRequest\",\n        trace.WithAttributes(\n            attribute.String(\"http.method\", r.Method),\n            attribute.String(\"http.url\", r.URL.String()),\n        ),\n    )\n    defer span.End()\n\n    // Database operation\n    ctx, dbSpan := tracer.Start(ctx, \"database.query\")\n    result, err := queryDatabase(ctx, \"SELECT * FROM users\")\n    dbSpan.SetAttributes(\n        attribute.Int(\"db.rows_affected\", len(result)),\n    )\n    if err != nil {\n        dbSpan.RecordError(err)\n        dbSpan.SetStatus(codes.Error, err.Error())\n    }\n    dbSpan.End()\n\n    // External API call\n    ctx, apiSpan := tracer.Start(ctx, \"external.api.call\")\n    apiResult := callExternalAPI(ctx)\n    apiSpan.End()\n\n    w.WriteHeader(http.StatusOK)\n    json.NewEncoder(w).Encode(map[string]interface{}{\n        \"data\": result,\n        \"api\": apiResult,\n    })\n}\n```\n\n## DevSecOps: Security as Code\n\nGüvenlik, DevOps pipeline'ının ayrılmaz bir parçası olmalı.\n\n### 1. Container Security Scanning\n\n```yaml\n# Trivy security scanning in CI/CD\nsecurity-scan:\n  stage: security\n  image: aquasec/trivy:latest\n  script:\n    # Scan Docker image\n    - trivy image --severity HIGH,CRITICAL \n        --exit-code 1 \n        --no-progress \n        --format template \n        --template \"@contrib/gitlab.tpl\" \n        -o gl-container-scanning-report.json \n        $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    \n    # Scan IaC files\n    - trivy config --severity HIGH,CRITICAL \n        --exit-code 1 \n        terraform/\n    \n    # Scan dependencies\n    - trivy fs --security-checks vuln,config \n        --exit-code 1 \n        .\n  artifacts:\n    reports:\n      container_scanning: gl-container-scanning-report.json\n```\n\n### 2. Policy as Code with OPA\n\n```rego\n# Kubernetes admission control policies\npackage kubernetes.admission\n\nimport future.keywords.contains\nimport future.keywords.if\nimport future.keywords.in\n\n# Deny containers without